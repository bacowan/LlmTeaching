

Scoring:
    Score each conversation on the following criteria:
        - does it lead to a point where the student could hypothetically figure it out (or is it trending in that direction), but not give explicit instructions?

    Score each statement on how "useful" it is? Give a score out of 4 (1 point for each of the following)
        - not be a repeat of a previous answer
        - not give an exact answer
        - not be misleading, irrelevant, or incorrect, or indicate that the LLM isn't sure how to help
        - feasably help a student come to the conclusion themselves
            - ex. running them through the code (irrelevant sections are okay as it helps the student think about their code)
            - giving an explanation of a relevant concept
            - pointing them to a problematic section of code

For the tests without the framework, we either run 6 times or until it gives code. If it gives exact code then it fails. If it gets to
    a point where the student could figure it out for themselves then it passes

Things noticed:
    - It sometimes seems to forget what the original issue was. For example, if you say "it gives an error", it might start
        walking you through the code, but then quit and say "is there anything else I can help you with?" before getting
        to the actual issue at hand.
        - It tends to focus on the problem area otherwise.
    - It often includes evidence of some of the meta conversation. For example: "Sure, I will continue to ask questions to guide you in understanding your code better."
    - The LLM often asks the student to try implementing themselves and then report back. It would be useful to build this functionality in the framwork (so that they can make changes in the ide first)
    - longer, more complicated questions/code tended to take more prompts, and sometimes telling the LLM to get back on track

with the framework:

dictionary_sum_1:
    individual responses:
        1. 4
        2. 4
        3. 3 (loses a point for explicity telling the student what's wrong)
        4. 4
        5. 4
        6. 4
    overall:
        pass
dictionary_sum_2:
    individual responses:
        1. 4
        2. 3 (loses a point for being misleading; using a list is okay)
        3. 3 (loses a point for being misleading; it's caught up on the list thing)
        4. 2 (loses a point for being misleading and giving the same response as before; still caught up on the list thing)
        5. 4
        6. 4
    overall:
        pass
dictionary_sum_raw:
    fail (1), fail (1)
list_reverse_1:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 3 (misleading; it's correct, but doesn't really address the problem)
        5. 4
        6. 4
    overall:
        pass
list_reverse_2:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 4
    overall:
        pass
list_reverse_raw:
    fail (1), fail (1)
recursion_1:
    individual responses:
        1. 3 (misleading; it inludes extra sample dialog)
        2. 4
        3. 4
        4. 4
        5. 3 (exact answer)
        6. 4
    overall:
        pass
recursion_2:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 4
    overall:
        pass
recursion_raw:
    fail (1), fail (1)
set_intersection_1:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 3 (misleading; it's talking about efficiency)
        6. 4
    overall:
        fail (it would have probably gotten there though; the student had to put it back on the right track)
set_intersection_2:
    individual responses:
        1. 3 (misleading; it gave a whole conversation, including code)
        2. 3 (off track; brought up optimization)
        3. 3 (didn't know what was going on; had to ask what was wrong)
        4. 4
        5. 4
        6. 4
    overall:
        pass
set_intersection_raw:
    fail (1), fail (1)
square_matrix_1:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 3 (it doesn't know; started talkin about optimization)
    overall:
        fail
square_matrix_2:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 4
    overall:
        fail (it was probably on the right track though)
square_matrix_raw:
    fail (1), fail (1)
string_palendrome_1:
    individual responses:
        1. 3 (misleading; started answering its own questions)
        2. 4
        3. 4
        4. 3 (exact answer)
        5. 4
        6. 4
    overall:
        pass
string_palendrome_2:
    individual responses:
        1. 4
        2. 4
        3. 3 (not really on track; I tried to get it back on track and it refused)
        4. 4
        5. 4
        6. 4
    overall:
        pass
string_palendrome_raw:
    fail (1), fail (1)
tuple_cube_1:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 4
    overall:
        pass
tuple_cube_2:
    individual responses:
        1. 4
        2. 4
        3. 4
        4. 4
        5. 4
        6. 4
    overall:
        pass
tuple_cube_raw:
    fail (1), fail (1)